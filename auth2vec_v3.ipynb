{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1PPhgocrHf7"
      },
      "source": [
        "v3: use \"crorisId\" as the primary identifier for authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpvj_rYbRlPG",
        "outputId": "4f10305d-4e9e-4306-c8b0-6ce145531020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting colbert-ai\n",
            "  Downloading colbert_ai-0.2.21-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting triton\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting pylate\n",
            "  Downloading pylate-1.1.2.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Collecting bitarray (from colbert-ai)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting datasets (from colbert-ai)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai) (2.2.5)\n",
            "Collecting git-python (from colbert-ai)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
            "Collecting python-dotenv (from colbert-ai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai) (1.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from colbert-ai) (4.44.2)\n",
            "Collecting ujson (from colbert-ai)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Collecting sentence-transformers==3.0.1 (from pylate)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.10/dist-packages (from pylate) (0.34.2)\n",
            "Collecting voyager>=2.0.9 (from pylate)\n",
            "  Downloading voyager-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting sqlitedict>=2.1.0 (from pylate)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas>=2.2.1 (from pylate)\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1->pylate) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1->pylate) (1.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1->pylate) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1->pylate) (10.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->pylate) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->pylate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.31.0->pylate) (0.4.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets->colbert-ai)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->colbert-ai)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets->colbert-ai)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.1->pylate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.1->pylate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.1->pylate) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Collecting gitpython (from git-python->colbert-ai)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1->pylate) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.1->pylate) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1->pylate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1->pylate) (3.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1->pylate) (3.5.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers==3.0.1->pylate) (1.3.0)\n",
            "Downloading torch_geometric-2.6.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colbert_ai-0.2.21-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading voyager-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pylate, sqlitedict\n",
            "  Building wheel for pylate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylate: filename=pylate-1.1.2-py3-none-any.whl size=45613 sha256=ff7d607bbb6c0da18ddde4464bf711ad86431ebebd27295afa56a5602c292336\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/c5/c2/3ba9822ef39c8573d5472dd332ea1901f5cfbcce291567dbee\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=e3e6fe06e2317d81eb1eda8ea2e5386848fc93923b44060390a7b17ddcb0e1a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built pylate sqlitedict\n",
            "Installing collected packages: sqlitedict, ninja, bitarray, xxhash, voyager, ujson, triton, smmap, python-dotenv, pyarrow, nltk, dill, pandas, multiprocess, gitdb, gitpython, torch_geometric, git-python, sentence-transformers, datasets, pylate, colbert-ai\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitarray-2.9.2 colbert-ai-0.2.21 datasets-3.0.0 dill-0.3.8 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.43 multiprocess-0.70.16 ninja-1.11.1.1 nltk-3.9.1 pandas-2.2.3 pyarrow-17.0.0 pylate-1.1.2 python-dotenv-1.0.1 sentence-transformers-3.0.1 smmap-5.0.1 sqlitedict-2.1.0 torch_geometric-2.6.0 triton-3.0.0 ujson-5.10.0 voyager-2.0.9 xxhash-3.5.0\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187309225 sha256=237ef9c6157db394e1ddde4ba609a21ebb98382377a27041edc09318801a6f24\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.6.3\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch_geometric nltk -U einops colbert-ai triton pylate\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdw-f33cRh8A",
        "outputId": "1af40780-cc8d-4252-fc55-bc3d1102091b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import pyarrow.parquet as pq\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from pylate import models, indexes, retrieve\n",
        "import json\n",
        "import ast\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    \"\"\"Preprocess a list of texts.\"\"\"\n",
        "    preprocessed_texts = []\n",
        "    for text in tqdm(texts, desc=\"Preprocessing texts\"):\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "        tokens = [stemmer.stem(word) for word in text.split() if word not in stop_words]\n",
        "        preprocessed_texts.append(' '.join(tokens))\n",
        "    return preprocessed_texts\n",
        "\n",
        "def extract_author_info(osobeResources):\n",
        "    author_info = []\n",
        "    if isinstance(osobeResources, str):\n",
        "        try:\n",
        "            osobeResources = ast.literal_eval(osobeResources)\n",
        "        except (ValueError, SyntaxError):\n",
        "            print(f\"Failed to parse osobeResources: {osobeResources[:200]}...\")\n",
        "            return author_info\n",
        "\n",
        "    if isinstance(osobeResources, dict):\n",
        "        osobe = osobeResources.get('_embedded', {}).get('osobe', [])\n",
        "        for osoba in osobe:\n",
        "            author_info.append({\n",
        "                'crorisId': osoba.get('crorisId'),\n",
        "                'ime': osoba.get('ime'),\n",
        "                'prezime': osoba.get('prezime')\n",
        "            })\n",
        "    elif osobeResources is not None:\n",
        "        print(f\"Unexpected type for osobeResources: {type(osobeResources)}\")\n",
        "        print(f\"Content: {str(osobeResources)[:200]}...\")\n",
        "    return author_info\n",
        "\n",
        "def load_data(parquet_file):\n",
        "    try:\n",
        "        table = pq.read_table(parquet_file)\n",
        "        df = table.to_pandas()\n",
        "        filtered_df = df[df['tip'] == \"izvorni znanstveni rad\"]\n",
        "        data_list = filtered_df.to_dict('records')\n",
        "\n",
        "        for item in data_list:\n",
        "            item['author_info'] = extract_author_info(item.get('osobeResources'))\n",
        "\n",
        "        print(f\"Finished reading Parquet file. Loaded {len(df)} publications\")\n",
        "        print(f\"After filtering for 'izvorni znanstveni rad': {len(data_list)} publications\")\n",
        "\n",
        "        # Print sample data for debugging\n",
        "        print(\"\\nSample of data:\")\n",
        "        for item in data_list[:5]:\n",
        "            print(f\"Title: {item.get('naslov', '')}\")\n",
        "            print(f\"Authors: {item.get('author_info', [])}\")\n",
        "            print(f\"osobeResources type: {type(item.get('osobeResources', ''))}\")\n",
        "            print(f\"osobeResources content: {str(item.get('osobeResources', ''))[:200]}...\")  # Print first 200 characters\n",
        "            print(\"---\")\n",
        "\n",
        "        return data_list\n",
        "    except FileNotFoundError:\n",
        "        print(\"fullPublications.parquet not found. Please ensure the file exists in the current directory.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error while loading the data: {str(e)}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def create_collection(data_list):\n",
        "    \"\"\"Create a collection from the data list for indexing.\"\"\"\n",
        "    collection = []\n",
        "    for item in tqdm(data_list, desc=\"Creating collection\"):\n",
        "        pid = item.get('id', '')\n",
        "        naslov = item.get('naslov', '')\n",
        "        sazeci = item.get('sazeci', [])\n",
        "        if isinstance(sazeci, list):\n",
        "            sazeci = ' '.join([s.get('naziv', '') for s in sazeci if isinstance(s, dict)])\n",
        "        text = f\"{naslov} {sazeci}\"\n",
        "        collection.append((pid, text))\n",
        "    return collection\n",
        "\n",
        "def save_graph(G, filename):\n",
        "    nx.write_gexf(G, get_drive_path(filename))\n",
        "    print(f\"Graph saved to {filename}\")\n",
        "\n",
        "def save_embeddings(embeddings, filename):\n",
        "    np.save(get_drive_path(filename), embeddings)\n",
        "    print(f\"Embeddings saved to {filename}\")\n",
        "\n",
        "def build_coauthorship_graph(data_list):\n",
        "    print(\"Building co-authorship graph\")\n",
        "    G = nx.Graph()\n",
        "    author_info = {}  # Dictionary to store author information\n",
        "\n",
        "    for item in tqdm(data_list, desc=\"Processing publications\"):\n",
        "        authors = item.get('author_info', [])\n",
        "        for author in authors:\n",
        "            crorisId = author.get('crorisId')\n",
        "            if crorisId:\n",
        "                if crorisId not in G:\n",
        "                    G.add_node(crorisId)\n",
        "                # Store author information\n",
        "                author_info[str(crorisId)] = {\n",
        "                    'ime': author.get('ime', ''),\n",
        "                    'prezime': author.get('prezime', '')\n",
        "                }\n",
        "\n",
        "        author_ids = [author.get('crorisId') for author in authors if author.get('crorisId')]\n",
        "        for i in range(len(author_ids)):\n",
        "            for j in range(i+1, len(author_ids)):\n",
        "                author1 = author_ids[i]\n",
        "                author2 = author_ids[j]\n",
        "                if G.has_edge(author1, author2):\n",
        "                    G[author1][author2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(author1, author2, weight=1)\n",
        "\n",
        "    print(f\"Co-authorship graph built. Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
        "    print(f\"Author info collected for {len(author_info)} authors\")\n",
        "\n",
        "    # Save author_info to a JSON file\n",
        "    with open(get_drive_path('author_info.json'), 'w') as f:\n",
        "        json.dump(author_info, f)\n",
        "    print(\"Author info saved to author_info.json\")\n",
        "\n",
        "    return G, author_info\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def reduce_dataset(data_list, strategy='random', sample_size=100, **kwargs):\n",
        "    \"\"\"Reduce the dataset size based on the chosen strategy.\"\"\"\n",
        "    if strategy == 'random':\n",
        "        return random.sample(data_list, min(sample_size, len(data_list)))\n",
        "    elif strategy == 'publication_type':\n",
        "        publication_type = kwargs.get('publication_type', 'izvorni znanstveni rad')\n",
        "        filtered = [item for item in data_list if item.get('tip') == publication_type]\n",
        "        return filtered[:sample_size]\n",
        "    elif strategy == 'time_based':\n",
        "        start_date = kwargs.get('start_date')\n",
        "        end_date = kwargs.get('end_date')\n",
        "        filtered = [item for item in data_list if start_date <= item.get('date') <= end_date]\n",
        "        return filtered[:sample_size]\n",
        "    elif strategy == 'author_count':\n",
        "        min_authors = kwargs.get('min_authors', 1)\n",
        "        max_authors = kwargs.get('max_authors', float('inf'))\n",
        "        filtered = [item for item in data_list if min_authors <= len(item.get('author_info', [])) <= max_authors]\n",
        "        return filtered[:sample_size]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy. Choose 'random', 'publication_type', 'time_based', or 'author_count'.\")\n",
        "\n",
        "def create_publication_embeddings(data_list, model, coauthorship_graph, author_info):\n",
        "    print(\"Starting publication embedding creation process\")\n",
        "\n",
        "    try:\n",
        "        texts = [f\"{item.get('naslov', '')} {' '.join([s.get('naziv', '') for s in item.get('sazeci', []) if isinstance(s, dict)])}\" for item in tqdm(data_list, desc=\"Preparing texts\")]\n",
        "\n",
        "        print(f\"Texts to encode: {texts[:2]}...\")  # Print first two texts\n",
        "        print(f\"Number of texts: {len(texts)}\")\n",
        "\n",
        "        # Implement batching\n",
        "        batch_size = 1024  # Adjust this based on your available memory\n",
        "        publication_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            try:\n",
        "                batch_embeddings = model.encode(batch_texts)\n",
        "\n",
        "                if batch_embeddings is None:\n",
        "                    print(f\"Warning: model.encode() returned None for batch {i//batch_size + 1}\")\n",
        "                    print(f\"Batch texts: {batch_texts}\")\n",
        "                    continue\n",
        "\n",
        "                # Handle the case where batch_embeddings is a list\n",
        "                if isinstance(batch_embeddings, list):\n",
        "                    batch_embeddings = np.array(batch_embeddings)\n",
        "\n",
        "                # If the embeddings are 3D (batch_size, sequence_length, embedding_dim),\n",
        "                # average over the sequence length dimension\n",
        "                if len(batch_embeddings.shape) == 3:\n",
        "                    batch_embeddings = batch_embeddings.mean(axis=1)\n",
        "\n",
        "                if isinstance(batch_embeddings, torch.Tensor):\n",
        "                    batch_embeddings = batch_embeddings.cpu().numpy()\n",
        "\n",
        "                publication_embeddings.append(batch_embeddings)\n",
        "                print(f\"Batch {i//batch_size + 1} shape: {batch_embeddings.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error encoding batch {i//batch_size + 1}: {str(e)}\")\n",
        "                print(f\"Batch texts: {batch_texts}\")\n",
        "\n",
        "        if not publication_embeddings:\n",
        "            raise ValueError(\"All batches failed to encode\")\n",
        "\n",
        "        publication_embeddings = np.concatenate(publication_embeddings, axis=0)\n",
        "        print(f\"Final publication embeddings shape: {publication_embeddings.shape}\")\n",
        "\n",
        "        coauthorship_graph, author_info = build_coauthorship_graph(data_list)\n",
        "        author_to_index = {author: i for i, author in enumerate(coauthorship_graph.nodes())}\n",
        "\n",
        "        edge_index = torch.tensor([[author_to_index[edge[0]], author_to_index[edge[1]]] for edge in coauthorship_graph.edges()]).t().contiguous()\n",
        "\n",
        "        x = torch.zeros((len(author_to_index), publication_embeddings.shape[1]))\n",
        "        author_publication_counts = {author: 0 for author in author_to_index}\n",
        "        for idx, item in enumerate(data_list):\n",
        "            for author_info in item.get('author_info', []):\n",
        "                crorisId = author_info.get('crorisId')\n",
        "                if crorisId in author_to_index:\n",
        "                    x[author_to_index[crorisId]] += torch.tensor(publication_embeddings[idx], dtype=torch.float)\n",
        "                    author_publication_counts[crorisId] += 1\n",
        "\n",
        "        for author, count in author_publication_counts.items():\n",
        "            if count > 0:\n",
        "                x[author_to_index[author]] /= count\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "        input_dim = x.shape[1]\n",
        "        hidden_dim = 128\n",
        "        gnn_model = GNN(input_dim, hidden_dim)\n",
        "\n",
        "        optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "        num_epochs = 100\n",
        "        for epoch in tqdm(range(num_epochs), desc=\"Training GNN\"):\n",
        "            optimizer.zero_grad()\n",
        "            out = gnn_model(data.x, data.edge_index)\n",
        "            loss = F.mse_loss(out, data.x)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            final_embeddings = gnn_model(data.x, data.edge_index).cpu().numpy()\n",
        "\n",
        "        author_final_embeddings = {author: final_embeddings[idx] for author, idx in author_to_index.items()}\n",
        "\n",
        "        print(\"Author embedding creation completed\")\n",
        "        return author_final_embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the author embedding creation process: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def visualize_embeddings(author_embeddings, author_info, n_components=2, perplexity=30, n_iter=1000):\n",
        "    print(\"Starting t-SNE visualization...\")\n",
        "    embeddings = np.array(list(author_embeddings.values()))\n",
        "    author_ids = list(author_embeddings.keys())\n",
        "\n",
        "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, verbose=1)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(20, 16))\n",
        "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
        "\n",
        "    # Add labels for a subset of points (e.g., top 100 authors)\n",
        "    top_authors = sorted(author_publication_counts.items(), key=lambda x: x[1], reverse=True)[:100]\n",
        "    for crorisId, _ in top_authors:\n",
        "        idx = author_ids.index(crorisId)\n",
        "        plt.annotate(author_info.get(crorisId, str(crorisId)),\n",
        "                     (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
        "                     xytext=(5, 2), textcoords='offset points', ha='right', va='bottom',\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
        "                     arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    plt.title('t-SNE visualization of author embeddings')\n",
        "    plt.xlabel('t-SNE feature 1')\n",
        "    plt.ylabel('t-SNE feature 2')\n",
        "    plt.savefig(get_drive_path('author_embeddings_tsne.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"t-SNE visualization completed and saved.\")\n",
        "\n",
        "def calculate_author_scores(data_list):\n",
        "    author_scores = {}\n",
        "    for item in data_list:\n",
        "        authors = item.get('author_info', [])\n",
        "        if authors:\n",
        "            weight = 1.0 / len(authors)\n",
        "            for author in authors:\n",
        "                crorisId = author.get('crorisId')\n",
        "                if crorisId:\n",
        "                    author_scores[crorisId] = author_scores.get(crorisId, 0) + weight\n",
        "    return author_scores\n",
        "\n",
        "def get_author_name(author_info, crorisId):\n",
        "    #print(f\"Debug: author_info type: {type(author_info)}\")\n",
        "    #print(f\"Debug: author_info content: {author_info}\")\n",
        "    #print(f\"crorisId: {crorisId}\")\n",
        "\n",
        "    if isinstance(author_info, dict):\n",
        "        author_data = author_info.get(str(crorisId))\n",
        "        if isinstance(author_data, dict):\n",
        "            ime = author_data.get('ime', '')\n",
        "            prezime = author_data.get('prezime', '')\n",
        "            if ime and prezime:\n",
        "                return f\"{ime} {prezime}\"\n",
        "    return f\"Unknown (crorisId: {crorisId})\"\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def find_max_authors_publication(data_list):\n",
        "    max_authors = 0\n",
        "    max_pub = None\n",
        "    for item in data_list:\n",
        "        authors = item.get('author_info', [])\n",
        "        if len(authors) > max_authors:\n",
        "            max_authors = len(authors)\n",
        "            max_pub = item\n",
        "    return max_pub, max_authors\n",
        "\n",
        "def plot_author_count_distribution(data_list):\n",
        "    author_counts = [len(item.get('author_info', [])) for item in data_list]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(author_counts, bins=50, edgecolor='black')\n",
        "    plt.title('Distribution of Author Counts')\n",
        "    plt.xlabel('Number of Authors')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.savefig(get_drive_path('author_count_distribution.png'))\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting author-to-vector process\")\n",
        "\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Set the path to the auth2vec folder in your Google Drive\n",
        "        auth2vec_folder = \"/content/drive/My Drive/auth2vec\"\n",
        "\n",
        "        # Create a function to get a path in the auth2vec folder\n",
        "        def get_drive_path(filename):\n",
        "            return os.path.join(auth2vec_folder, filename)\n",
        "\n",
        "        # Set the path to the parquet file\n",
        "        parquet_file = os.path.join(auth2vec_folder, \"fullPublications.parquet\")\n",
        "\n",
        "        # Load the Jina-ColBERT model using pylate\n",
        "        print(\"Loading Jina-ColBERT model...\")\n",
        "        model = models.ColBERT(\n",
        "            model_name_or_path=\"jinaai/jina-colbert-v2\",\n",
        "            query_prefix=\"[QueryMarker]\",\n",
        "            document_prefix=\"[DocumentMarker]\",\n",
        "            attend_to_expansion_tokens=True,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        print(f\"Model type: {type(model)}\")\n",
        "        print(f\"Model attributes: {dir(model)}\")\n",
        "\n",
        "        # Load data\n",
        "        print(\"Loading data...\")\n",
        "        data_list = load_data(parquet_file)\n",
        "        print(f\"Full dataset size: {len(data_list)}\")\n",
        "\n",
        "        # Call this function after loading the data\n",
        "        max_pub, max_authors = find_max_authors_publication(data_list)\n",
        "        print(f\"Publication with most authors: {max_pub.get('naslov', 'No title')}\")\n",
        "        print(f\"Maximum number of authors: {max_authors}\")\n",
        "\n",
        "        # Add this check after loading the data\n",
        "        max_authors_publication = max(data_list, key=lambda x: len(x.get('author_info', [])))\n",
        "        print(f\"Max authors in a single publication: {len(max_authors_publication.get('author_info', []))}\")\n",
        "        print(f\"Title of max authors publication: {max_authors_publication.get('naslov', 'No title')}\")\n",
        "\n",
        "        plot_author_count_distribution(data_list)\n",
        "\n",
        "        print(\"Sample of data:\")\n",
        "        for item in data_list[:5]:\n",
        "            print(f\"Title: {item.get('naslov', '')}\")\n",
        "            print(f\"Authors: {item.get('author_info', [])}\")\n",
        "            print(f\"Abstract: {item.get('sazeci', '')[:100]}...\")\n",
        "            print(\"---\")\n",
        "\n",
        "        # Print sample of data\n",
        "        print(\"Sample of data:\")\n",
        "        for item in tqdm(data_list[:2], desc=\"Printing sample\"):\n",
        "            print(f\"Title: {item.get('naslov', '')}\")\n",
        "            print(f\"Authors: {item.get('author_info', '')}\")\n",
        "            print(f\"Abstract: {item.get('sazeci', '')[:100]}...\")  # Print first 100 characters of abstract\n",
        "            print(\"---\")\n",
        "\n",
        "        # Create author embeddings\n",
        "        print(\"Starting author embedding creation...\")\n",
        "        coauthorship_graph, author_info = build_coauthorship_graph(data_list)\n",
        "\n",
        "        if coauthorship_graph.number_of_nodes() == 0:\n",
        "            print(\"The co-authorship graph is empty. Unable to create author embeddings.\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        author_embeddings = create_publication_embeddings(data_list, model, coauthorship_graph, author_info)\n",
        "        if author_embeddings is None:\n",
        "            print(\"Failed to create author embeddings. Exiting.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Author embeddings created. Number of authors: {len(author_embeddings)}\")\n",
        "        save_embeddings(author_embeddings, \"author_embeddings.npy\")\n",
        "\n",
        "        # Convert author embeddings to numpy arrays\n",
        "        print(\"Converting embeddings to numpy arrays...\")\n",
        "        author_embeddings = {author: np.array(embedding) for author, embedding in tqdm(author_embeddings.items(), desc=\"Converting embeddings\")}\n",
        "\n",
        "        # Save the embeddings\n",
        "        print(\"Saving embeddings...\")\n",
        "        np.save(get_drive_path('author_embeddings.npy'), np.array(list(author_embeddings.values())))\n",
        "\n",
        "        visualize_embeddings(author_embeddings, author_info)\n",
        "\n",
        "        print(\"Embeddings saved and visualization created.\")\n",
        "\n",
        "        # Additional analysis: Most prolific authors\n",
        "        author_publication_counts = {}\n",
        "        for item in data_list:\n",
        "            for author in item.get('author_info', []):\n",
        "                crorisId = author.get('crorisId')\n",
        "                if crorisId:\n",
        "                    author_publication_counts[crorisId] = author_publication_counts.get(crorisId, 0) + 1\n",
        "\n",
        "        author_scores = calculate_author_scores(data_list)\n",
        "\n",
        "        top_authors = sorted(author_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        print(\"\\nTop 10 most prolific authors (weighted by number of co-authors):\")\n",
        "        for crorisId, score in top_authors:\n",
        "            author_name = get_author_name(author_info, crorisId)\n",
        "            print(f\"{crorisId} - {author_name}: {score:.2f} weighted publications\")\n",
        "\n",
        "        # Additional analysis: Co-authorship network statistics\n",
        "        print(\"\\nCo-authorship network statistics:\")\n",
        "        print(f\"Number of nodes (authors): {coauthorship_graph.number_of_nodes()}\")\n",
        "        print(f\"Number of edges (co-authorships): {coauthorship_graph.number_of_edges()}\")\n",
        "        print(f\"Network density: {nx.density(coauthorship_graph):.4f}\")\n",
        "\n",
        "        largest_cc = max(nx.connected_components(coauthorship_graph), key=len)\n",
        "        print(f\"Size of the largest connected component: {len(largest_cc)}\")\n",
        "\n",
        "        # Add author count statistics\n",
        "        author_counts = [len(item.get('author_info', [])) for item in data_list if item.get('author_info')]\n",
        "        author_count_stats = {\n",
        "            \"mean\": np.mean(author_counts),\n",
        "            \"median\": np.median(author_counts),\n",
        "            \"min\": np.min(author_counts),\n",
        "            \"max\": np.max(author_counts),\n",
        "            \"95th_percentile\": np.percentile(author_counts, 95)\n",
        "        }\n",
        "\n",
        "        print(\"\\nUpdated Author count statistics:\")\n",
        "        for stat, value in author_count_stats.items():\n",
        "            print(f\"{stat}: {value:.2f}\")\n",
        "\n",
        "        # Update the results dictionary:\n",
        "        results = {\n",
        "            \"top_authors\": [\n",
        "                (\n",
        "                    str(crorisId),\n",
        "                    float(score),\n",
        "                    get_author_name(author_info, crorisId)\n",
        "                ) for crorisId, score in top_authors\n",
        "            ],\n",
        "            \"network_stats\": {\n",
        "                \"nodes\": int(coauthorship_graph.number_of_nodes()),\n",
        "                \"edges\": int(coauthorship_graph.number_of_edges()),\n",
        "                \"density\": float(nx.density(coauthorship_graph)),\n",
        "                \"largest_cc_size\": int(len(largest_cc))\n",
        "            },\n",
        "            \"author_count_stats\": {\n",
        "                key: float(value) if isinstance(value, np.number) else value\n",
        "                for key, value in author_count_stats.items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(get_drive_path('analysis_results.json'), 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=convert_to_serializable)\n",
        "\n",
        "        print(\"Analysis results saved to analysis_results.json\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"Author-to-vector process completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpDGKILU-24G",
        "outputId": "9a1ebcf1-82a8-4bc0-fe6d-0b99e4e1e0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of publications: 225302\n",
            "Improved dataset statistics have been calculated and visualized.\n",
            "Check the following PNG files for the plots:\n",
            "- publication_dates.png\n",
            "- publication_years_histogram.png\n",
            "- authors_distribution.png\n",
            "- authors_boxplot.png\n",
            "Year range: 2004 to 2026\n",
            "Median number of authors per publication: 3.0\n",
            "Maximum number of authors in a single publication: 331.0\n",
            "Author Collaboration Patterns:\n",
            "- Median number of authors per publication: 3.00\n",
            "- Mean number of authors per publication: 3.91\n",
            "- Standard deviation: 7.90\n",
            "- 25th percentile: 1.00\n",
            "- 75th percentile: 4.00\n",
            "- Maximum number of authors on a single publication: 331\n",
            "- Interquartile range: 3.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "\n",
        "def load_data(parquet_file):\n",
        "    table = pq.read_table(parquet_file)\n",
        "    df = table.to_pandas()\n",
        "    return df\n",
        "\n",
        "def calculate_dataset_statistics(df):\n",
        "    total_publications = len(df)\n",
        "    print(f\"Total number of publications: {total_publications}\")\n",
        "\n",
        "    df['godina'] = pd.to_datetime(df['godina'], format='%Y', errors='coerce')\n",
        "    date_distribution = df['godina'].dt.year.value_counts().sort_index()\n",
        "\n",
        "    df['num_authors'] = df['autori'].str.split(';').str.len()\n",
        "    author_distribution = df['num_authors'].value_counts().sort_index()\n",
        "\n",
        "    return date_distribution, author_distribution\n",
        "\n",
        "def plot_publication_dates(date_distribution):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    date_distribution.plot(kind='line')\n",
        "    plt.title('Distribution of Publication Dates')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('publication_dates.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_publication_years_histogram(df):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Get the range of years\n",
        "    min_year = df['godina'].dt.year.min()\n",
        "    max_year = df['godina'].dt.year.max()\n",
        "\n",
        "    # Create bins for each year\n",
        "    bins = range(min_year, max_year + 2)  # +2 to include the last year\n",
        "\n",
        "    # Plot the histogram with specified bins\n",
        "    df['godina'].dt.year.hist(bins=bins, edgecolor='black', align='left')\n",
        "\n",
        "    plt.title('Histogram of Publication Years')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Set x-ticks to show every 5 years\n",
        "    plt.xticks(range(min_year, max_year + 1, 5), rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('publication_years_histogram.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_authors_distribution(author_distribution):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    author_distribution[author_distribution.index <= 20].plot(kind='bar', logy=True)\n",
        "    plt.title('Distribution of Number of Authors per Publication (up to 20)')\n",
        "    plt.xlabel('Number of Authors')\n",
        "    plt.ylabel('Count (log scale)')\n",
        "    plt.xticks(range(len(author_distribution[author_distribution.index <= 20])),\n",
        "               [int(i) for i in author_distribution[author_distribution.index <= 20].index],\n",
        "               rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('authors_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_authors_boxplot(df):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x=df['num_authors'])\n",
        "    plt.title('Boxplot of Number of Authors per Publication')\n",
        "    plt.xlabel('Number of Authors')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('authors_boxplot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parquet_file = os.path.join(auth2vec_folder, \"fullPublications.parquet\")\n",
        "    df = load_data(parquet_file)\n",
        "\n",
        "    date_distribution, author_distribution = calculate_dataset_statistics(df)\n",
        "\n",
        "    plot_publication_dates(date_distribution)\n",
        "    plot_publication_years_histogram(df)\n",
        "    plot_authors_distribution(author_distribution)\n",
        "    plot_authors_boxplot(df)\n",
        "\n",
        "    print(\"Improved dataset statistics have been calculated and visualized.\")\n",
        "    print(\"Check the following PNG files for the plots:\")\n",
        "    print(\"- publication_dates.png\")\n",
        "    print(\"- publication_years_histogram.png\")\n",
        "    print(\"- authors_distribution.png\")\n",
        "    print(\"- authors_boxplot.png\")\n",
        "\n",
        "    # Print additional insights\n",
        "    print(f\"Year range: {date_distribution.index.min()} to {date_distribution.index.max()}\")\n",
        "    print(f\"Median number of authors per publication: {df['num_authors'].median()}\")\n",
        "    print(f\"Maximum number of authors in a single publication: {df['num_authors'].max()}\")\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean_authors = df['num_authors'].mean()\n",
        "    std_authors = df['num_authors'].std()\n",
        "\n",
        "    # Calculate percentiles\n",
        "    percentiles = df['num_authors'].quantile([0.25, 0.75])\n",
        "    p25 = percentiles[0.25]\n",
        "    p75 = percentiles[0.75]\n",
        "\n",
        "    # Calculate median (already done in your code, but included here for completeness)\n",
        "    median_authors = df['num_authors'].median()\n",
        "\n",
        "    # Maximum number of authors (already calculated, but included here for completeness)\n",
        "    max_authors = df['num_authors'].max()\n",
        "\n",
        "    # Print results\n",
        "    print(\"Author Collaboration Patterns:\")\n",
        "    print(f\"- Median number of authors per publication: {median_authors:.2f}\")\n",
        "    print(f\"- Mean number of authors per publication: {mean_authors:.2f}\")\n",
        "    print(f\"- Standard deviation: {std_authors:.2f}\")\n",
        "    print(f\"- 25th percentile: {p25:.2f}\")\n",
        "    print(f\"- 75th percentile: {p75:.2f}\")\n",
        "    print(f\"- Maximum number of authors on a single publication: {max_authors:.0f}\")\n",
        "\n",
        "    # If you want to calculate the interquartile range (IQR)\n",
        "    iqr = p75 - p25\n",
        "    print(f\"- Interquartile range: {iqr:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFSg8F4JEkct",
        "outputId": "0830c077-f4ee-443f-8709-461046783a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Co-authorship Network Analysis\n",
            "\n",
            "1. Network Statistics:\n",
            "Number of nodes: 25582\n",
            "Number of edges: 223419\n",
            "Average degree: 17.47\n",
            "Network density: 0.0007\n",
            "Number of connected components: 112\n",
            "\n",
            "4. Community Detection and Visualization:\n",
            "Number of communities detected: 143\n",
            "Co-authorship Network Analysis completed. Visualizations saved.\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from community import community_louvain\n",
        "import seaborn as sns\n",
        "\n",
        "def coauthorship_network_analysis(G):\n",
        "    print(\"Starting Co-authorship Network Analysis\")\n",
        "\n",
        "    # 1. Network statistics\n",
        "    print(\"\\n1. Network Statistics:\")\n",
        "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
        "    avg_degree = sum(dict(G.degree()).values()) / G.number_of_nodes()\n",
        "    print(f\"Average degree: {avg_degree:.2f}\")\n",
        "    print(f\"Network density: {nx.density(G):.4f}\")\n",
        "    print(f\"Number of connected components: {nx.number_connected_components(G)}\")\n",
        "\n",
        "    # 2. Degree distribution plot\n",
        "    degrees = [d for n, d in G.degree()]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(degrees, bins=50)\n",
        "    plt.title(\"Degree Distribution\")\n",
        "    plt.xlabel(\"Degree\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.savefig(\"degree_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Community detection and visualization\n",
        "    print(\"\\n4. Community Detection and Visualization:\")\n",
        "\n",
        "    # Detect communities\n",
        "    communities = community_louvain.best_partition(G)\n",
        "\n",
        "    # Count the number of communities\n",
        "    num_communities = len(set(communities.values()))\n",
        "    print(f\"Number of communities detected: {num_communities}\")\n",
        "\n",
        "    # Visualize the network with communities (for a subset of nodes if the network is large)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "\n",
        "    # If the network is too large, visualize only a subset\n",
        "    if G.number_of_nodes() > 1000:\n",
        "        subG = G.subgraph(list(G.nodes())[:1000])\n",
        "    else:\n",
        "        subG = G\n",
        "\n",
        "    pos = nx.spring_layout(subG)\n",
        "\n",
        "    # Color nodes based on community\n",
        "    colors = [communities[node] for node in subG.nodes()]\n",
        "\n",
        "    nx.draw(subG, pos, node_color=colors, with_labels=False, node_size=30, cmap=plt.cm.rainbow)\n",
        "    plt.title(\"Co-authorship Network Communities\")\n",
        "    plt.savefig(\"coauthorship_communities.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Visualize community size distribution\n",
        "    community_sizes = [len([n for n in G.nodes() if communities[n] == c]) for c in set(communities.values())]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(community_sizes, kde=True)\n",
        "    plt.title(\"Community Size Distribution\")\n",
        "    plt.xlabel(\"Community Size\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.savefig(\"community_size_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Co-authorship Network Analysis completed. Visualizations saved.\")\n",
        "\n",
        "# Assuming you have already built the co-authorship graph G\n",
        "coauthorship_network_analysis(coauthorship_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI4AJKQb6dvi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "author_embeddings_file = os.path.join(auth2vec_folder, \"author_embeddings.npy\")\n",
        "embeddings = np.load(author_embeddings_file, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4HufM596gq3",
        "outputId": "526bd130-5ec7-450f-a9e0-0515636af681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(18319, 1.0000002),\n",
              " (23554, 1.0000002),\n",
              " (11432, 1.0000001),\n",
              " (11156, 1.0000001),\n",
              " (24025, 1.0000001)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_similar_authors(author_index, top_n=5):\n",
        "    similarities = cosine_similarity([embeddings[author_index]], embeddings)[0]\n",
        "    most_similar = similarities.argsort()[-top_n-1:-1][::-1]\n",
        "    return [(i, similarities[i]) for i in most_similar]\n",
        "\n",
        "# Example usage:\n",
        "similar_authors = find_similar_authors(0)  # Find authors similar to the first author\n",
        "similar_authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxRzinDW6vXU",
        "outputId": "ad18e3b1-96b2-46e5-8ff4-43e5df9008f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=10)  # Adjust the number of clusters as needed\n",
        "clusters = kmeans.fit_predict(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq14la5o65-8",
        "outputId": "3f40105f-d604-4411-81f9-d892be859869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5 1 1 ... 8 4 8]\n"
          ]
        }
      ],
      "source": [
        "print(clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "3CHqki8m66ib",
        "outputId": "0fdb6beb-08d6-43ad-d3df-6789f790993c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Perform LDA for topic modeling\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "topic_distribution = lda.fit_transform(np.abs(list(author_embeddings.values())))  # Using absolute values\n",
        "\n",
        "# Assign authors to topics\n",
        "author_topics = {}\n",
        "for i, author in enumerate(author_embeddings.keys()):\n",
        "    top_topic = topic_distribution[i].argmax()\n",
        "    author_topics[author] = top_topic\n",
        "\n",
        "# Print authors for each topic\n",
        "for topic in range(n_topics):\n",
        "    topic_authors = [author for author, author_topic in author_topics.items() if author_topic == topic]\n",
        "    print(f\"Topic {topic}: {len(topic_authors)} authors\")\n",
        "    print(f\"Sample authors: {', '.join(get_author_name(author_info, author) for author in topic_authors[:5])}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhvyWsEpbN8b",
        "outputId": "e673570b-7a00-4b38-b38d-24c3e9d40fdc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 6456 authors\n",
            "Sample authors: Filip Šuligoj, Dragan Gamberger, Josip Peranić, Snježana Mihalić Arbanas, Željko Arbanas\n",
            "\n",
            "Topic 1: 7972 authors\n",
            "Sample authors: Ivan Prskalo, Zoran Krupka, Isabella Matticchio, Damjan Pelc, Marin Lukas\n",
            "\n",
            "Topic 2: 3081 authors\n",
            "Sample authors: Branimir Vukosav, Marijana Kresić Vukosav, Boris Badurina, Sanda Dubravčić-Šimunjak, Petra Rajić Šikanjić\n",
            "\n",
            "Topic 3: 4 authors\n",
            "Sample authors: Siniša Radović, Maja Mitrović, Matea Dvorščak, Nives Rumenjak\n",
            "\n",
            "Topic 4: 4777 authors\n",
            "Sample authors: Hrvoje Jurić, Ivana Šutej, Darko Božić, Kristina Peroš, Darije Plančak\n",
            "\n",
            "Topic 5: 1395 authors\n",
            "Sample authors: Tena Šimunjak, Vesna Boraska, Mirjana Babić Leko, Magda Sindičić, Božidar Šantek\n",
            "\n",
            "Topic 6: 0 authors\n",
            "Sample authors: \n",
            "\n",
            "Topic 7: 0 authors\n",
            "Sample authors: \n",
            "\n",
            "Topic 8: 0 authors\n",
            "Sample authors: \n",
            "\n",
            "Topic 9: 3279 authors\n",
            "Sample authors: Monika Mladenović, Divna Krpan, Đurđana Ozretić Došen, Cathy-Theresa Kolega, Ante Punda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Perform K-means clustering\n",
        "n_clusters = 10\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(list(author_embeddings.values()))\n",
        "\n",
        "# Assign authors to clusters\n",
        "author_clusters = {}\n",
        "for i, author in enumerate(author_embeddings.keys()):\n",
        "    author_clusters[author] = cluster_labels[i]\n",
        "\n",
        "# Print authors for each cluster\n",
        "for cluster in range(n_clusters):\n",
        "    cluster_authors = [author for author, author_cluster in author_clusters.items() if author_cluster == cluster]\n",
        "    print(f\"Cluster {cluster}: {len(cluster_authors)} authors\")\n",
        "    print(f\"Sample authors: {', '.join(get_author_name(author_info, author) for author in cluster_authors[:5])}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRhBKH-xcFRK",
        "outputId": "89bfa725-8e16-4d47-aed2-d59d4852b96e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: 4395 authors\n",
            "Sample authors: Stjepko Krehula, Svetozar Musić, Tena Šimunjak, Veselin Škrabić, Vesna Boraska\n",
            "\n",
            "Cluster 1: 1522 authors\n",
            "Sample authors: Branimir Vukosav, Marijana Kresić Vukosav, Anita Rapan Papeša, Mirjana Sanader, Boško Pešić\n",
            "\n",
            "Cluster 2: 1453 authors\n",
            "Sample authors: Hrvoje Jurić, Ivana Šutej, Darije Plančak, Željko Verzak, Damir Sekulić\n",
            "\n",
            "Cluster 3: 1120 authors\n",
            "Sample authors: Tanja Šestanj-Perić, Miroslav Bistrović, Danijel Carev, Damir Pavlović, Daša Bosanac\n",
            "\n",
            "Cluster 4: 940 authors\n",
            "Sample authors: Ivan Perić, Boris Badurina, Ivana Fojs, Kristina Detelj, Ljubica Ranogajec\n",
            "\n",
            "Cluster 5: 2469 authors\n",
            "Sample authors: Monika Mladenović, Divna Krpan, Đurđana Ozretić Došen, Mario Šestan, Nataša Kovačić\n",
            "\n",
            "Cluster 6: 3446 authors\n",
            "Sample authors: Filip Šuligoj, Dragan Gamberger, Mirna Gilman Ranogajec, Vlasta Roška, Dalida Rittossa\n",
            "\n",
            "Cluster 7: 764 authors\n",
            "Sample authors: Željko Heimer, Antun Husinec, TOMISLAV BILIĆ, Nina Lončar, Maša Surić\n",
            "\n",
            "Cluster 8: 7377 authors\n",
            "Sample authors: Ivan Prskalo, Josip Peranić, Snježana Mihalić Arbanas, Željko Arbanas, Zoran Krupka\n",
            "\n",
            "Cluster 9: 3478 authors\n",
            "Sample authors: Darko Božić, Kristina Peroš, Barbara Gilić, Arben Beriša, Ivan Krešimir Lukić\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "def load_embeddings(file_path):\n",
        "    return np.load(file_path, allow_pickle=True)\n",
        "\n",
        "def get_author_name(author_info, author_id):\n",
        "    author_data = author_info.get(str(author_id))\n",
        "    if author_data:\n",
        "        return f\"{author_data.get('ime', '')} {author_data.get('prezime', '')}\".strip()\n",
        "    return f\"Unknown (ID: {author_id})\"\n",
        "\n",
        "# Load embeddings and author info\n",
        "author_embeddings = load_embeddings(get_drive_path('author_embeddings.npy'))\n",
        "with open(get_drive_path('author_info.json'), 'r') as f:\n",
        "    author_info = json.load(f)\n",
        "\n",
        "# Create a sorted list of author_info IDs\n",
        "sorted_author_info_ids = sorted(author_info.keys(), key=int)\n",
        "\n",
        "# Create a mapping between sequential IDs and author_info IDs\n",
        "id_mapping = {i: author_id for i, author_id in enumerate(sorted_author_info_ids)}\n",
        "\n",
        "# Create aligned embeddings and author info\n",
        "aligned_embeddings = OrderedDict()\n",
        "aligned_author_info = OrderedDict()\n",
        "\n",
        "for i, emb in enumerate(author_embeddings):\n",
        "    if i < len(id_mapping):\n",
        "        author_id = id_mapping[i]\n",
        "        aligned_embeddings[author_id] = emb\n",
        "        aligned_author_info[author_id] = author_info[author_id]\n",
        "\n",
        "print(f\"Total aligned authors: {len(aligned_embeddings)}\")\n",
        "\n",
        "# Print sample of aligned authors\n",
        "print(\"\\nSample of aligned authors:\")\n",
        "for i, (author_id, emb) in enumerate(aligned_embeddings.items()):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    author_name = get_author_name(aligned_author_info, author_id)\n",
        "    print(f\"ID: {author_id}, Name: {author_name}, Embedding shape: {emb.shape}\")\n",
        "\n",
        "# Check for any remaining mismatches\n",
        "embedding_ids = set(aligned_embeddings.keys())\n",
        "author_info_ids = set(aligned_author_info.keys())\n",
        "\n",
        "missing_in_embeddings = author_info_ids - embedding_ids\n",
        "missing_in_author_info = embedding_ids - author_info_ids\n",
        "\n",
        "print(f\"\\nIDs in aligned author_info but not in aligned embeddings: {len(missing_in_embeddings)}\")\n",
        "print(f\"IDs in aligned embeddings but not in aligned author_info: {len(missing_in_author_info)}\")\n",
        "\n",
        "# Now you can proceed with your clustering and analysis using the aligned_embeddings and aligned_author_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0dp0vyqiNP5",
        "outputId": "fd6ef920-523f-48ee-c0d9-45dc58b27044"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total aligned authors: 26964\n",
            "\n",
            "Sample of aligned authors:\n",
            "ID: 1, Name: Petra Udovičić, Embedding shape: (128,)\n",
            "ID: 4, Name: Lucija Vusić, Embedding shape: (128,)\n",
            "ID: 23, Name: Ivana Mandić Hekman, Embedding shape: (128,)\n",
            "ID: 24, Name: Gordan Šarić, Embedding shape: (128,)\n",
            "ID: 25, Name: Davor Vagić, Embedding shape: (128,)\n",
            "ID: 26, Name: Predrag Pale, Embedding shape: (128,)\n",
            "ID: 28, Name: Anica Hunjet, Embedding shape: (128,)\n",
            "ID: 31, Name: Milena Žic Fuchs, Embedding shape: (128,)\n",
            "ID: 32, Name: Sanja Biškup, Embedding shape: (128,)\n",
            "ID: 33, Name: Davor Tomas, Embedding shape: (128,)\n",
            "\n",
            "IDs in aligned author_info but not in aligned embeddings: 0\n",
            "IDs in aligned embeddings but not in aligned author_info: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "def load_embeddings(file_path):\n",
        "    return np.load(file_path, allow_pickle=True)\n",
        "\n",
        "def get_author_name(author_info, author_id):\n",
        "    author_data = author_info.get(str(author_id), {})\n",
        "    return f\"{author_data.get('ime', '')} {author_data.get('prezime', '')}\".strip() or f\"Unknown (ID: {author_id})\"\n",
        "\n",
        "# Load embeddings and author info\n",
        "author_embeddings = load_embeddings(get_drive_path('author_embeddings.npy'))\n",
        "with open(get_drive_path('author_info.json'), 'r') as f:\n",
        "    author_info = json.load(f)\n",
        "\n",
        "# Create a sorted list of author_info IDs\n",
        "sorted_author_info_ids = sorted(author_info.keys(), key=int)\n",
        "\n",
        "# Create a mapping between sequential IDs and author_info IDs\n",
        "id_mapping = {i: author_id for i, author_id in enumerate(sorted_author_info_ids)}\n",
        "\n",
        "# Create aligned embeddings and author info\n",
        "aligned_embeddings = OrderedDict()\n",
        "aligned_author_info = OrderedDict()\n",
        "\n",
        "for i, emb in enumerate(author_embeddings):\n",
        "    if i < len(id_mapping):\n",
        "        author_id = id_mapping[i]\n",
        "        aligned_embeddings[author_id] = emb\n",
        "        aligned_author_info[author_id] = author_info[author_id]\n",
        "\n",
        "# Prepare data for clustering\n",
        "authors = list(aligned_embeddings.keys())\n",
        "embeddings = np.array(list(aligned_embeddings.values()))\n",
        "\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# Perform K-means clustering\n",
        "n_clusters = min(10, len(authors))\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Analyze clusters\n",
        "for i in range(n_clusters):\n",
        "    cluster_authors = [authors[j] for j in range(len(authors)) if cluster_labels[j] == i]\n",
        "    print(f\"Cluster {i}: {len(cluster_authors)} authors\")\n",
        "    # Print some random sample authors from each cluster\n",
        "    sample_size = min(5, len(cluster_authors))\n",
        "    sample_authors = random.sample(cluster_authors, sample_size)\n",
        "    sample_names = [get_author_name(aligned_author_info, author) for author in sample_authors]\n",
        "    print(f\"Sample authors: {', '.join(sample_names)}\")\n",
        "    print()\n",
        "\n",
        "# Visualize clusters using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='viridis')\n",
        "plt.colorbar(scatter)\n",
        "plt.title('t-SNE visualization of author clusters')\n",
        "plt.savefig(get_drive_path('author_clusters_tsne.png'))\n",
        "plt.close()\n",
        "\n",
        "print(\"Cluster visualization saved as 'author_clusters_tsne.png'\")\n",
        "\n",
        "# Analyze cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "for i, centroid in enumerate(centroids):\n",
        "    distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
        "    closest_author_idx = np.argmin(distances)\n",
        "    closest_author = authors[closest_author_idx]\n",
        "    author_name = get_author_name(aligned_author_info, closest_author)\n",
        "    print(f\"Cluster {i} centroid closest author: {author_name}\")\n",
        "\n",
        "# Find most central authors\n",
        "distances_to_centroid = kmeans.transform(embeddings)\n",
        "central_authors = [authors[np.argmin(distances_to_centroid[:, i])] for i in range(n_clusters)]\n",
        "print(\"\\nMost central authors for each cluster:\")\n",
        "for i, author in enumerate(central_authors):\n",
        "    author_name = get_author_name(aligned_author_info, author)\n",
        "    print(f\"Cluster {i}: {author_name}\")\n",
        "\n",
        "# Analyze inter-cluster distances\n",
        "inter_cluster_distances = np.linalg.norm(centroids[:, None] - centroids, axis=2)\n",
        "np.fill_diagonal(inter_cluster_distances, np.inf)\n",
        "closest_clusters = np.argmin(inter_cluster_distances, axis=1)\n",
        "print(\"\\nClosest cluster pairs:\")\n",
        "for i, closest in enumerate(closest_clusters):\n",
        "    if i < closest:\n",
        "        print(f\"Cluster {i} and Cluster {closest}\")\n",
        "\n",
        "# Additional analysis: Top authors in each cluster\n",
        "print(\"\\nTop 5 authors in each cluster:\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_authors = [authors[j] for j in range(len(authors)) if cluster_labels[j] == i]\n",
        "    cluster_distances = distances_to_centroid[cluster_labels == i, i]\n",
        "    top_authors = sorted(zip(cluster_authors, cluster_distances), key=lambda x: x[1])[:5]\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "    for author, distance in top_authors:\n",
        "        author_name = get_author_name(aligned_author_info, author)\n",
        "        print(f\"  {author_name} (distance from centroid: {distance:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5uPGMFbdDjt",
        "outputId": "e8752b9d-bee3-4c9c-b649-028066ef961b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (26964, 128)\n",
            "Cluster 0: 4395 authors\n",
            "Sample authors: Tomo Lucijanić, Kiran Ram Porikam Poil, Asja Tukić, Tomislav Gregurić, Ivan Palijan\n",
            "\n",
            "Cluster 1: 1522 authors\n",
            "Sample authors: Marina Martinčević, Elinor Trogrlic, Krešimir Kužić, Kristina Hodak, Tina Barun\n",
            "\n",
            "Cluster 2: 1453 authors\n",
            "Sample authors: Silvija Šiljeg, Marija Pejčinović Burić, Nenad Smoljan, Matija Gredičak, Tonći Erceg\n",
            "\n",
            "Cluster 3: 1120 authors\n",
            "Sample authors: Vanja Miljković, Lovro Turkalj, Lucija Zenić, Kristina Bučar, Petar Šušnjara\n",
            "\n",
            "Cluster 4: 940 authors\n",
            "Sample authors: Matej Nakić, Tea Petković, Marko Blažević, Gordan Kompes, Ivica Čatić\n",
            "\n",
            "Cluster 5: 2469 authors\n",
            "Sample authors: Ana Vrgoč, Tanja Roje-Bonacci, Vladimir Lončarević, Danijela Lugarić Vukas, Tošo Maršić\n",
            "\n",
            "Cluster 6: 3446 authors\n",
            "Sample authors: Marija Malnar Jurišić, Biljana Činčurak Erceg, Ante Batistić, Goran Miljić, Dragan Jukić\n",
            "\n",
            "Cluster 7: 764 authors\n",
            "Sample authors: Valentin Vidić, Mirela Jukić Bokun, Nikolina Grković, Tomislav Domazet-Lošo, Hermes Rene Belusca\n",
            "\n",
            "Cluster 8: 7377 authors\n",
            "Sample authors: Monika Mladenović, Patricija Hršak, Domagoj Karačić, Miroslav Tuđman, Luka Keller\n",
            "\n",
            "Cluster 9: 3478 authors\n",
            "Sample authors: Vesna Rastija, Zvonimir Bokulić, Ksenija Grubišić, Dino Šojat, Marko Močibob\n",
            "\n",
            "Cluster visualization saved as 'author_clusters_tsne.png'\n",
            "Cluster 0 centroid closest author: Alena Gizdić\n",
            "Cluster 1 centroid closest author: Natalija Matić\n",
            "Cluster 2 centroid closest author: Luka Manojlović\n",
            "Cluster 3 centroid closest author: Nino Antulov-Fantulin\n",
            "Cluster 4 centroid closest author: Ante Blaće\n",
            "Cluster 5 centroid closest author: Ana Ježovita\n",
            "Cluster 6 centroid closest author: Ante Batistić\n",
            "Cluster 7 centroid closest author: Ana Čizmić Grbić\n",
            "Cluster 8 centroid closest author: Anja Dvojković\n",
            "Cluster 9 centroid closest author: Tomislav Gregorić\n",
            "\n",
            "Most central authors for each cluster:\n",
            "Cluster 0: Alena Gizdić\n",
            "Cluster 1: Natalija Matić\n",
            "Cluster 2: Luka Manojlović\n",
            "Cluster 3: Nino Antulov-Fantulin\n",
            "Cluster 4: Ante Blaće\n",
            "Cluster 5: Ana Ježovita\n",
            "Cluster 6: Ante Batistić\n",
            "Cluster 7: Ana Čizmić Grbić\n",
            "Cluster 8: Anja Dvojković\n",
            "Cluster 9: Tomislav Gregorić\n",
            "\n",
            "Closest cluster pairs:\n",
            "Cluster 0 and Cluster 8\n",
            "Cluster 1 and Cluster 5\n",
            "Cluster 2 and Cluster 9\n",
            "Cluster 3 and Cluster 6\n",
            "Cluster 4 and Cluster 5\n",
            "Cluster 5 and Cluster 8\n",
            "Cluster 6 and Cluster 8\n",
            "Cluster 7 and Cluster 8\n",
            "\n",
            "Top 5 authors in each cluster:\n",
            "\n",
            "Cluster 0:\n",
            "  Alena Gizdić (distance from centroid: 0.0028)\n",
            "  Anto Jonjić (distance from centroid: 0.0031)\n",
            "  Petar Vukelić (distance from centroid: 0.0037)\n",
            "  Sanda Ročak (distance from centroid: 0.0039)\n",
            "  Vladimir Sertić (distance from centroid: 0.0042)\n",
            "\n",
            "Cluster 1:\n",
            "  Natalija Matić (distance from centroid: 0.0102)\n",
            "  Ivan Krešo (distance from centroid: 0.0105)\n",
            "  Denis Šipuš (distance from centroid: 0.0128)\n",
            "  Marina Đukić (distance from centroid: 0.0129)\n",
            "  Hrvojka Soljačić Vraneš (distance from centroid: 0.0132)\n",
            "\n",
            "Cluster 2:\n",
            "  Luka Manojlović (distance from centroid: 0.0034)\n",
            "  Rajna Knez (distance from centroid: 0.0034)\n",
            "  Jagoda Makjanić (distance from centroid: 0.0035)\n",
            "  Marcel Prelogović (distance from centroid: 0.0053)\n",
            "  Radomir Ječmenica (distance from centroid: 0.0057)\n",
            "\n",
            "Cluster 3:\n",
            "  Nino Antulov-Fantulin (distance from centroid: 0.0067)\n",
            "  Ana Ramljak (distance from centroid: 0.0069)\n",
            "  Iris Zavoreo (distance from centroid: 0.0070)\n",
            "  Radoslav Katičić (distance from centroid: 0.0071)\n",
            "  Saša Dešić (distance from centroid: 0.0083)\n",
            "\n",
            "Cluster 4:\n",
            "  Ante Blaće (distance from centroid: 0.0105)\n",
            "  Lucija Matković (distance from centroid: 0.0114)\n",
            "  Miljana Kukić (distance from centroid: 0.0129)\n",
            "  Mirna Batistić (distance from centroid: 0.0145)\n",
            "  Filip Stević (distance from centroid: 0.0148)\n",
            "\n",
            "Cluster 5:\n",
            "  Ana Ježovita (distance from centroid: 0.0118)\n",
            "  Marina Olujić Tomazin (distance from centroid: 0.0119)\n",
            "  Branka Kunović (distance from centroid: 0.0122)\n",
            "  Ivan Brkić (distance from centroid: 0.0124)\n",
            "  Marko Erceg (distance from centroid: 0.0131)\n",
            "\n",
            "Cluster 6:\n",
            "  Ante Batistić (distance from centroid: 0.0052)\n",
            "  Marija Davidović (distance from centroid: 0.0054)\n",
            "  Ivana Kovačević (distance from centroid: 0.0059)\n",
            "  Vesna Boraska (distance from centroid: 0.0059)\n",
            "  Tomislav Šarić (distance from centroid: 0.0059)\n",
            "\n",
            "Cluster 7:\n",
            "  Ana Čizmić Grbić (distance from centroid: 0.0077)\n",
            "  Nikola Vukman (distance from centroid: 0.0082)\n",
            "  Robert Blagoni (distance from centroid: 0.0093)\n",
            "  Ozren Jakšić (distance from centroid: 0.0100)\n",
            "  Danijel Kanski (distance from centroid: 0.0108)\n",
            "\n",
            "Cluster 8:\n",
            "  Anja Dvojković (distance from centroid: 0.0044)\n",
            "  Mario Matković (distance from centroid: 0.0049)\n",
            "  Ivan Šumiga (distance from centroid: 0.0053)\n",
            "  Teo Bratinčević (distance from centroid: 0.0054)\n",
            "  Robert Bacalja (distance from centroid: 0.0054)\n",
            "\n",
            "Cluster 9:\n",
            "  Tomislav Gregorić (distance from centroid: 0.0018)\n",
            "  Ariana Vorko-Jović (distance from centroid: 0.0022)\n",
            "  Ruža Sabočanec (distance from centroid: 0.0024)\n",
            "  Tomislav Čop (distance from centroid: 0.0037)\n",
            "  Viktor Šunde (distance from centroid: 0.0044)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}